{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import civicweb_scraper\n",
    "\n",
    "import requests\n",
    "from requests_cache import CachedSession\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from csv import DictReader\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/Modify Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = civicweb_scraper.create_cache(\n",
    "    name=\"test_cache\", \n",
    "    expire_after=3600*24*14, \n",
    "    allowable_codes=[200] # only save successful requests\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOGGER SETUP\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s:%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.setLevel(logging.DEBUG)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all websites with domain `civicweb.net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as a giant dictionary\n",
    "# get the existing list of scraped subdomains\n",
    "# read in dataframe\n",
    "\n",
    "# try:\n",
    "#     existing_subdomains_df = pd.read_csv(civicweb_scraper.OUT_FOLDER / \"civicweb_subdomains.csv\", index_col=[0])\n",
    "    \n",
    "#     subdomains_dict = {\n",
    "#         subdomain[\"subdomain\"]: {\n",
    "#             key: subdomain[key] \n",
    "#             for key in existing_subdomains_df.columns\n",
    "#             if key != \"subdomain\"\n",
    "#         } for i, subdomain in existing_subdomains_df.iterrows()\n",
    "#     }\n",
    "# except FileNotFoundError:\n",
    "#     existing_subdomains_df = pd.DataFrame()\n",
    "#     subdomains_dict = {}\n",
    "\n",
    "# subdomains_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing subdomain information if available\n",
    "try:\n",
    "    google_scrape_results = json.load(open(f\"{civicweb_scraper.OUT_FOLDER}/subdomains.json\"))\n",
    "    logger.info(f\"Loaded in existing subdomains.json file.\")\n",
    "except:\n",
    "    google_scrape_results = {}\n",
    "    logger.info(f\"No existing subdomains.json file found in the {civicweb_scraper.OUT_FOLDER} folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom google search engine by following https://developers.google.com/custom-search/docs/tutorial/creatingcse\n",
    "GOOGLE_SEARCH_ENDPOINT = \"https://www.googleapis.com/customsearch/v1?\"\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API')\n",
    "GOOGLE_SEARCH_ENGINE_ID = os.getenv('GOOGLE_SEARCH_ENGINE_ID')\n",
    "\n",
    "# google_params = {\n",
    "#     \"key\": GOOGLE_API_KEY, \n",
    "#     \"cx\":GOOGLE_SEARCH_ENGINE_ID,\n",
    "#     \"q\":\"site:civicweb.net\",\n",
    "#     \"num\":10,\n",
    "#     \"gl\":\"ca\"\n",
    "# }\n",
    "\n",
    "# response = session.get(GOOGLE_SEARCH_ENDPOINT, params=google_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.raise_for_status()\n",
    "# search_results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_scrape_results = []\n",
    "\n",
    "# google_scrape_results.extend([\n",
    "#     {key:item[key] for key in ['title', 'link', 'snippet']} \n",
    "#     for item in search_results[\"items\"]])\n",
    "\n",
    "# google_scrape_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 0\n",
    "max_page = 15\n",
    "num_results_per_page = 10\n",
    "for page_index in range(start_page, max_page):\n",
    "    # time.sleep(1) # wait for 1 second between each search\n",
    "    start_index = page_index*10+1\n",
    "    # search with query and result page\n",
    "    logger.info(f\"Scraping page {page_index} with results from {start_index} to {start_index+num_results_per_page-1}...\")\n",
    "\n",
    "    google_params = {\n",
    "        \"key\": GOOGLE_API_KEY, \n",
    "        \"cx\":GOOGLE_SEARCH_ENGINE_ID,\n",
    "        \"q\":\"site:civicweb.net\",\n",
    "        \"num\":num_results_per_page,\n",
    "        'start': start_index,\n",
    "        \"gl\":\"ca\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(GOOGLE_SEARCH_ENDPOINT, params=google_params)\n",
    "        response.raise_for_status()\n",
    "        search_results = response.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(e)\n",
    "        continue\n",
    "\n",
    "    for item in search_results[\"items\"]:\n",
    "        logger.debug(f\"Looking at {item['link']}\")\n",
    "        if \".civicweb.net\" in item[\"link\"]:\n",
    "            subdomain = item[\"link\"].split(\".civicweb.net\")[0].split(\"https://\")[-1]\n",
    "            if subdomain in google_scrape_results:\n",
    "                logger.debug(f\"Already seen {subdomain}\")\n",
    "                continue\n",
    "            else:\n",
    "                logger.info(f\"Adding {subdomain} to results list\")\n",
    "                google_scrape_results[subdomain] ={\n",
    "                    \"root_url\": f\"https://{subdomain}.civicweb.net\",\n",
    "                    \"google_search_url\": item[\"link\"],\n",
    "                    \"title\": item[\"title\"],\n",
    "                    \"description\": item[\"snippet\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_scrape_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_scrape_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Found {len(google_scrape_results)} unique subdomains from the Google Search API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the newly found subdomains\n",
    "with open(civicweb_scraper.OUT_FOLDER / f\"subdomains.json\", \"w\") as f:\n",
    "    json.dump(google_scrape_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape documents from each subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_document(session, document, root_url, subdomain) -> dict:\n",
    "#     ''' \n",
    "#     Download according to a document information dictionary, and return a dictionary of the download details.\n",
    "#     '''\n",
    "#     error = \"\"\n",
    "#     file_extension = \"\"\n",
    "#     file_type = \"\"\n",
    "\n",
    "#     try:\n",
    "#         response = session.get(url=root_url+document[\"url\"], headers=civicweb_scraper.HEADERS)\n",
    "\n",
    "#         out_path = civicweb_scraper.OUT_FOLDER.joinpath(subdomain, *document[\"parent\"])\n",
    "\n",
    "#         file_extension, file_type = civicweb_scraper.get_filetype(response)\n",
    "#         logger.debug((f\"File: {document['name']}{file_extension}\"))\n",
    "\n",
    "#         civicweb_scraper.download_file(\n",
    "#         response, \n",
    "#         filename=document[\"name\"]+file_extension,\n",
    "#         out_path=out_path)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         error += f\"{e}\"\n",
    "#     finally:\n",
    "#         download_dict = {\n",
    "#                     \"name\": document[\"name\"]+file_extension,\n",
    "#                     \"file_type\": file_type,\n",
    "#                     \"subdomain\": subdomain,\n",
    "#                     \"parent_path\": \"/\".join(document[\"parent\"]),\n",
    "#                     \"root_url\": root_url, \n",
    "#                     \"url\": document[\"url\"], \n",
    "#                     \"parent_url\": document[\"parent_url\"],\n",
    "#                     \"date_scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#                     \"error\": error\n",
    "#                 }\n",
    "#         return download_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subdomain in google_scrape_results.keys():\n",
    "\n",
    "for subdomain in ['dev', 'victoria', 'notl']:\n",
    "    # breadth-first search to find all subfolders and documents\n",
    "    logger.info(f\"Processing {subdomain}...\")\n",
    "    root_url = google_scrape_results[subdomain][\"root_url\"]\n",
    "\n",
    "    try:\n",
    "        # load existing document tracking files if they exist\n",
    "        with open(civicweb_scraper.OUT_FOLDER / f\"{subdomain}_documents.csv\",'r') as data:\n",
    "            dict_reader = DictReader(f)\n",
    "            documents = list(dict_reader)\n",
    "        logger.info(f\"Found existing document tracking file for {subdomain}: {len(done_folders)} folders completed and {len(folders)} to go\")\n",
    "    except:\n",
    "        documents = []\n",
    "    \n",
    "    try:\n",
    "        # load existing folder json deques if they exist\n",
    "        with open(civicweb_scraper.OUT_FOLDER / f\"{subdomain}_folders.json\", \"r\") as f: \n",
    "            json_data = json.load(f)\n",
    "            folders, done_folders = deque(json_data[\"folders\"]), deque(json_data[\"done_folders\"])\n",
    "\n",
    "        logger.info(f\"Found existing folder tracking file for {subdomain}: {len(done_folders)} folders completed and {len(folders)} to go\")\n",
    "    except FileNotFoundError:\n",
    "        logger.debug(f\"No existing folder tracking file for {subdomain}. Adding root folder to folders as a start.\")\n",
    "\n",
    "        folders = deque([]) \n",
    "        done_folders = deque([])\n",
    "\n",
    "        # get the root url for this subdomain and add the folders at the root to be processed\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url=root_url+\"/filepro/documents/\")\n",
    "            response.raise_for_status()\n",
    "            folders.extend(civicweb_scraper.get_items(response, parent_url=\"/filepro/documents/\",parent=[], is_folder=True))\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logger.error(f\"Unable to fetch website information for {root_url+'/filepro/documents/'}: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Exception occurred for URL {root_url+'/filepro/documents/'}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # process all documents on site\n",
    "    while len(folders)>0:\n",
    "        time.sleep(1)\n",
    "        logger.debug(\"num folders to visit: %s\", len([str(folder) for folder in folders]))\n",
    "        logger.debug(\"\\nnum completed folders: %s\", len([str(folder) for folder in done_folders]))\n",
    "        \n",
    "        curr_folder = folders.popleft()\n",
    "        logger.info(f\"\\nEntering folder {curr_folder['name']} at location \\'/{'/'.join(curr_folder['parent'])}\\'...\") \n",
    "        try:\n",
    "            response = session.get(url=root_url+curr_folder[\"url\"])\n",
    "\n",
    "            # update current folder's details\n",
    "            curr_path = curr_folder[\"parent\"].copy()\n",
    "            curr_path.append(curr_folder[\"name\"])\n",
    "\n",
    "            logger.debug(\"parent path to add to children folders/documents: %s\", curr_path)\n",
    "            \n",
    "            # add subfolders to visit from this folder to the folders deque\n",
    "            children_folders = civicweb_scraper.get_items(response, parent_url=curr_folder[\"url\"], parent=curr_path, is_folder=True)\n",
    "            folders.extend(children_folders)\n",
    "\n",
    "            # add documents to download from this folder\n",
    "            children_documents = civicweb_scraper.get_items(response, parent_url=curr_folder[\"url\"], parent=curr_path, is_folder=False)\n",
    "            # documents.extend(children_documents)\n",
    "\n",
    "            # download documents\n",
    "            for document in tqdm(children_documents):\n",
    "                logger.debug(f\"downloading document {document['name']} (URL: {document['url']})\")\n",
    "                download_dict = civicweb_scraper.download_document(session=session, document=document, root_url=root_url, subdomain=subdomain)\n",
    "                documents.append(download_dict)\n",
    "\n",
    "            logger.debug(str(documents))\n",
    "            \n",
    "        except Exception as e:\n",
    "            tb_str = ''.join(traceback.format_exception(e))\n",
    "            logger.error(tb_str)\n",
    "            continue\n",
    "        finally:\n",
    "            # save document information to a csv\n",
    "            logger.info(f\"Updating tracking files for {curr_folder['name']} with {len(children_folders)} new folders and {len(children_documents)} new documents.\")\n",
    "\n",
    "            out_df = pd.DataFrame(documents)\n",
    "            out_df.to_csv(civicweb_scraper.OUT_FOLDER / f\"{subdomain}_documents.csv\", index=False)\n",
    "            # save current folder/deque/documents information to a json file\n",
    "            with open(civicweb_scraper.OUT_FOLDER / f\"{subdomain}_folders.json\", \"w\") as f:\n",
    "                json.dump({\"folders\": list(folders),\n",
    "                           \"done_folders\": list(done_folders)}, f, indent=4)\n",
    "\n",
    "        done_folders.append(curr_folder)\n",
    "        curr_path = []\n",
    "        \n",
    "        logger.info(f\"... Exiting folder {curr_folder['name']}.\\n\")\n",
    "    logger.info(f\"... Found all documents for {subdomain}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cache_creation_time(url, session:CachedSession):\n",
    "#     response = session.get(url=url)\n",
    "#     return response.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_scrape_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()\n",
    "\n",
    "max_page_number = 10 # max number of pages to scrape\n",
    "num_results_per_page = 10 # default number\n",
    "\n",
    "for page_index in range(max_page_number):\n",
    "    time.sleep(1) # wait for 1 second between each search\n",
    "\n",
    "    # search with query and result page\n",
    "    start_index = 1+page_index*num_results_per_page\n",
    "    logger.info(f\"Scraping page {page_index+1} with results from {start_index} to {start_index+num_results_per_page-1}...\")\n",
    "\n",
    "    bing_url = f'https://www.bing.com/search?q=site%3acivicweb.net&first={start_index}' \n",
    "\n",
    "    # get url of each page result\n",
    "    try:\n",
    "        driver.get(bing_url)\n",
    "        # titles = driver.find_elements(by=By.CLASS_NAME, value=\"tptt\")\n",
    "        links = driver.find_elements(by=By.TAG_NAME, value=\"cite\")\n",
    "        # snippets = driver.find_elements(by=By.CLASS_NAME, value=\"b_lineclamp4\")\n",
    "\n",
    "        scraped_links.append(links.text)\n",
    "\n",
    "        # scraped_links.extend([\n",
    "        #     {\n",
    "        #         \"title\": title.text, \n",
    "        #         \"link\": link.text,\n",
    "        #         \"snippet\": snippet.text,\n",
    "        #     } for (title, link, snippet) in zip(titles, links, snippets)\n",
    "        # ])\n",
    "    except WebDriverException as e:\n",
    "        logger.error(\"Error opening Bing page for scraping: \", e)\n",
    "        continue\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "logger.info(f\"Bing scraping finished. Found {len(scraped_links)} links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_rows = [] # for the dataframe\n",
    "subdomains_with_documents = [] # valid site to be scraped\n",
    "\n",
    "for subdomain in subdomain_list:\n",
    "    root_url = f\"https://{subdomain}.civicweb.net\"\n",
    "    error = \"\"\n",
    "    # subdomain = root_url.split(\"https://\")[-1].split(\".civicweb.net\")[0]\n",
    "    # if subdomain returns code 200 and text is not empty, add to list of sites to scrape\n",
    "    try:\n",
    "        response = civicweb_scraper.fetch_webpage(url=root_url+\"/filepro/documents/\") # get the url of the documents hub\n",
    "        logger.debug(f\"{root_url} has a documents page, adding to valid subdomains list\")\n",
    "        subdomains_with_documents.append(subdomain)\n",
    "    except requests.HTTPError as e: \n",
    "        # error from requests module for url\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(f\"{response.status_code}: {e}\")\n",
    "        continue\n",
    "    except TypeError as e: # error from requests-cache \n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(f\"{response.status_code}: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(e)\n",
    "        continue\n",
    "    finally:\n",
    "        subdomain_info = {\n",
    "            \"subdomain\": subdomain, \n",
    "            \"root_url\": root_url, \n",
    "            \"bing_retrieval_date\": \"\",\n",
    "            \"'error'\": error,\n",
    "        }\n",
    "        subdomain_rows.append(subdomain_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_scrape_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scraped results without running selenium\n",
    "scraped_links = ['https://severn.civicweb.net', 'https://tucumcari.civicweb.net', 'https://williamsnd.civicweb.net', 'https://cocookmn.civicweb.net', 'https://greatermadawaska.civicweb.net', 'https://otonabeesouthmonaghan.civicweb.net', 'https://centrewellington.civicweb.net', 'https://google.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "civicweb_scraper.remove_url_from_cache(session, url=\"google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the website is valid subdomain of civicweb.net\n",
    "for link in scraped_links:\n",
    "    if '.civicweb.net' in link:\n",
    "        # get basic site information\n",
    "        subdomain = link.split(\".civicweb.net\")[0].split(\"https://\")[-1]\n",
    "        root_url = f\"https://{subdomain}.civicweb.net\"\n",
    "        documents_url = root_url+\"/filepro/documents/\"\n",
    "        error = \"\"\n",
    "\n",
    "        if overwrite_cache is True:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # response = civicweb_scraper.fetch_webpage(url=documents_url) \n",
    "            if session.cache.has_url(documents_url):\n",
    "                \n",
    "\n",
    "            response = civicweb_scraper.fetch_webpage_with_cache(url=documents_url, session=session) \n",
    "            # if subdomain in subdomains_dict:\n",
    "            #     # update existing subdomain with new scraping details\n",
    "            #     logger.debug(f\"{documents_url} has been retrieved before, adding to valid subdomains list\")\n",
    "\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            logger.error(f\"Check your connection. Unable to fetch {documents_url}: {e}\")\n",
    "            error += str(e)\n",
    "            continue\n",
    "\n",
    "        finally:\n",
    "            # update subdomains_dict with new scraping details\n",
    "            if subdomain in subdomains_dict:\n",
    "                subdomains_dict[subdomain].update(\n",
    "                    {\n",
    "                        \"bing_retrieval_date\": datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"),\"error\" : str(error), \n",
    "                        \"cached_date\": datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "                    })\n",
    "            else:\n",
    "                # add new subdomain with scraping details to the dictionary\n",
    "                subdomains_dict[subdomain] = {\n",
    "                    \"root_url\": root_url,\n",
    "                    \"bing_retrieval_date\": datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "                    \"error\": str(error),\n",
    "                    \"cached_date\": datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "                }\n",
    "        \n",
    "\n",
    "        # update with new details\n",
    "        # if subdomain in subdomains_dict:\n",
    "        #     subdomains[subdomain][\"bing_retrieval_date\"] = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        #     subdomains[subdomain][\"error\"] = e\n",
    "        # else:\n",
    "        #     subdomains[subdomain] = {\n",
    "        #         \"root_url\": root_url,\n",
    "        #         \"bing_retrieval_date\": datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "        #         \"error\": error\n",
    "        #     }\n",
    "        \n",
    "        #     subdomains_with_documents.append(subdomain)\n",
    "        #     scraped_links.append(subdomain)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for subdomains with a documents portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_rows = [] # for the dataframe\n",
    "subdomains_with_documents = [] # valid site to be scraped\n",
    "\n",
    "for subdomain in subdomain_list:\n",
    "    root_url = f\"https://{subdomain}.civicweb.net\"\n",
    "    error = \"\"\n",
    "    # subdomain = root_url.split(\"https://\")[-1].split(\".civicweb.net\")[0]\n",
    "    # if subdomain returns code 200 and text is not empty, add to list of sites to scrape\n",
    "    try:\n",
    "        response = civicweb_scraper.fetch_webpage(url=root_url+\"/filepro/documents/\") # get the url of the documents hub\n",
    "        logger.debug(f\"{root_url} has a documents page, adding to valid subdomains list\")\n",
    "        subdomains_with_documents.append(subdomain)\n",
    "    except requests.HTTPError as e: \n",
    "        # error from requests module for url\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(f\"{response.status_code}: {e}\")\n",
    "        continue\n",
    "    except TypeError as e: # error from requests-cache \n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(f\"{response.status_code}: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(f\"in {root_url}: {tb_str}\")\n",
    "        error += str(e)\n",
    "        continue\n",
    "    finally:\n",
    "        subdomain_info = {\n",
    "            \"subdomain\": subdomain, \n",
    "            \"root_url\": root_url, \n",
    "            \"bing_retrieval_date\": \"\",\n",
    "            \"'error'\": error,\n",
    "        }\n",
    "        subdomain_rows.append(subdomain_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track valid subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the existing list of scraped subdomains\n",
    "try:\n",
    "    existing_subdomains_df = pd.read_csv(civicweb_scraper.OUT_FOLDER / \"scraped_subdomains new.csv\", index_col=[0])\n",
    "except FileNotFoundError:\n",
    "    existing_subdomains_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subdomains_df = pd.DataFrame(subdomain_rows)\n",
    "new_subdomains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_subdomains_df = old_subdomains_df.reset_index(drop=True)\n",
    "new_subdomains_df = new_subdomains_df.reset_index(drop=True)\n",
    "\n",
    "updated_subdomains_df = pd.concat([old_subdomains_df, new_subdomains_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_subdomains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_subdomains_df.to_csv(civicweb_scraper.OUT_FOLDER / \"scraped_subdomains_test_updated.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "civicweb_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
