{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import logging\n",
    "# from joblib import Memory\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import mimetypes\n",
    "from datetime import datetime\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s:%(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests_cache\n",
    "\n",
    "requests_cache.install_cache('scraper_cache', backend='sqlite', expire_after=3600*24*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests_cache.clear() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cachedir = './cache' \n",
    "# memory = Memory(cachedir, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = \"https://pleasantontx.civicweb.net\"\n",
    "subdomain = root_url.split(\"https://\")[-1].split(\".civicweb.net\")[0]\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @memory.cache\n",
    "def fetch_webpage(url, headers=HEADERS):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return None\n",
    "    \n",
    "# def get_soup(url_append, root_url=root_url, headers=HEADERS):\n",
    "#     url = root_url + url_append\n",
    "#     # TODO: implement caching using joblib\n",
    "#     # cached_result = cache.get('html:%s' % url)\n",
    "#     # if cached_result:\n",
    "#     #     return cached_result\n",
    "#     print(f\"Getting BeautifulSoup object from {url}\")\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         # cache.set('html:%s' % url, page)\n",
    "#         bs = BeautifulSoup(response.content,'html.parser')\n",
    "#         return bs\n",
    "#     except Exception as e:\n",
    "#         print(\"Error: \", e)\n",
    "#         return None\n",
    "    \n",
    "# def get_item(bs:BeautifulSoup, items, parent_path=\"/\", is_folder=True):\n",
    "#     ''' \n",
    "#     parent_path (str):\n",
    "#         The path to the current folder, relative to the root of the website. Must start with a slash. At least one slash is required to indicate the root of the website.\n",
    "#     '''\n",
    "#     if is_folder:\n",
    "#         class_str = 'folder-link'\n",
    "#     else:\n",
    "#         class_str = 'file-link'\n",
    "\n",
    "#     for item in bs.find_all(\"a\", class_=class_str):\n",
    "#         if item.get('href') not in [i[\"url\"] for i in items]:\n",
    "#             item_info  = {\n",
    "#                 \"name\": item.text, \n",
    "#                 \"url\": item.get('href'), \n",
    "#                 \"parent_path\": parent_path\n",
    "#             }\n",
    "#             items.append(item_info)\n",
    "#     return items\n",
    "    \n",
    "def get_folders(bs:BeautifulSoup, folders:deque, parent_url:str, parent):\n",
    "    ''' \n",
    "    parents : list(str)\n",
    "        List of the folders on the path to the current folder, relative to the root of the website. parent_path[0] should be the first folder on the path, and parent_path[-1] is the current folder. parent_path=[] indicates that the folder belongs to the root.\n",
    "    '''\n",
    "    logger.debug(\"folder parents: %s\", parent)\n",
    "    for folder in bs.find_all(\"a\", class_='folder-link'):\n",
    "        folder_info = {\n",
    "                \"name\": folder.text.strip(), \n",
    "                \"url\": folder.get('href'),\n",
    "                \"parent\": parent,\n",
    "                \"parent_url\": parent_url\n",
    "            }\n",
    "        logger.debug(f\"{folder}\")\n",
    "\n",
    "        if folder.get('href') not in [f[\"url\"] for f in folders]:\n",
    "            logger.debug(\"Adding to folders deque to visit\")\n",
    "            folders.append(folder_info)\n",
    "    return folders\n",
    "\n",
    "def get_items(bs:BeautifulSoup, parent_url:str, parent=[], is_folder=True)->list[dict]:\n",
    "    ''' \n",
    "    Return a list of dictionaries containing information for each item in the current folder. Items are either documents or folders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bs: BeautifulSoup\n",
    "        The BeautifulSoup object to parse. Should be the documents site for a website under the civicweb.net domain.\n",
    "    parent_url : str\n",
    "        The url of the current folder. This is the url appended onto the root of the website, and should start with \"/\".\n",
    "    parent : list(str)\n",
    "        List of the folders on the path to the current item, relative to the root of the website. parent[0] should be the first folder on the path, and parent[-1] is the current folder. parent=[] indicates that the folder belongs to the root.\n",
    "    is_folder  : bool\n",
    "        Whether or not the current folder is a folder (True) or a document (False).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    items : list[dict]\n",
    "        A list of dictionaries containing information for each item in the current folder. Each dictionary contains the following keys:\n",
    "        - name : The name of the item.\n",
    "        - url : The url of the item.\n",
    "        - parent_url : The url of the parent folder.\n",
    "        - parent : A list of the folders on the path to the current item.\n",
    "    '''\n",
    "    logger.debug(\"folder parent: %s\", parent)\n",
    "\n",
    "    child_items = []\n",
    "    if is_folder: \n",
    "        item_class = \"folder-link\" \n",
    "    else: \n",
    "        item_class = \"document-link\" \n",
    "\n",
    "    for item in bs.find_all(\"a\", class_=item_class):\n",
    "        item_info = {\n",
    "                \"name\": item.text.strip(), \n",
    "                \"url\": item.get('href'),\n",
    "                \"parent\": parent,\n",
    "                \"parent_url\": parent_url\n",
    "            }\n",
    "        logger.debug(f\"Found {item_class} with name {item_info['name']} (url: {item_info['url']}) in directory at {'/'.join(item_info['parent'])} (url: {parent_url}).\")\n",
    "        child_items.append(item_info)\n",
    "    return child_items\n",
    "\n",
    "def get_documents(bs:BeautifulSoup, documents:list, parent_url:str, parent:list):\n",
    "    for doc in bs.find_all(\"a\", class_='document-link'):\n",
    "        document_info = {\n",
    "                \"name\": doc.text.strip(),\n",
    "                \"url\": doc[\"href\"],\n",
    "                \"parent\": parent,\n",
    "                \"parent_url\": parent_url\n",
    "            }\n",
    "        logger.debug(f\"Found document with name {document_info['name']} in {'/'.join(document_info['parent'])}, fetched from {document_info['url']}\")\n",
    "        if doc.get('href') not in [d[\"url\"] for d in documents]:\n",
    "            logger.debug(f\"Adding {document_info['name']} to documents list to download later\")\n",
    "            documents.append(document_info)\n",
    "    return documents\n",
    "\n",
    "def get_filetype(response:requests) -> tuple[str, str]:\n",
    "    ''' \n",
    "    Returns the extension and file type (MIME notation) of a response object based on its Content-Type header.\n",
    "    '''\n",
    "    header_mimetype = response.headers['Content-Type']\n",
    "\n",
    "    # remove mimetype parameter if it exists\n",
    "    if \";\" in header_mimetype:\n",
    "        header_mimetype = header_mimetype.split(\";\")[0]\n",
    "    extension = mimetypes.guess_extension(header_mimetype)\n",
    "\n",
    "    # raise error if file extension cannot be guessed from response\n",
    "    if extension is None or extension == \"\":\n",
    "        logger.warning(\"Could not determine file type for response\")\n",
    "        raise TypeError(f\"Could not determine file type\")\n",
    "    \n",
    "    return extension, header_mimetype\n",
    "\n",
    "# def format_output_path(parents:list, subdomain=\"\", out_folder=Path.cwd() / \"out\"):\n",
    "#     '''\n",
    "#     Formats the output path of the file to be downloaded. The output path will separate the file by subdomain and preserve the file structure from the scraped website.\n",
    "\n",
    "#     parents : list(str)\n",
    "#         The list of directories that the file is in from the root folder of the main website.\n",
    "#     subdomain : str, optional\n",
    "#         The subdomain of the website to be scraped from. Defaults to \"\". Used to separate scraped content into specific folders within the out_path.\n",
    "#     '''\n",
    "#     return out_folder.joinpath(subdomain, *parents)\n",
    "\n",
    "def download_file(response:requests, filename:str,  out_path=Path.cwd() / \"out\"):\n",
    "    ''' \n",
    "    Downloads a PDF file from the given response.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response : requests.Response\n",
    "        The response object.\n",
    "    filename : str\n",
    "        The name of the file being downloaded. Should include the extension.\n",
    "    \n",
    "    out_path : Path, optional\n",
    "        The path where the file should be saved on the disk, as a pathlib Path object. Defaults to a folder in the current working directory named \"out\". Does not need to exist.\n",
    "    '''\n",
    "    # out_path = Path(__file__).parent / out_path / subdomain / Path(*parents) / name + \".pdf\" # use Path.cwd() for Jupyter Notebook\n",
    "\n",
    "    out_path.mkdir(parents=True, exist_ok=True) # ensure directories leading up to the output file path exist\n",
    "    with open(out_path / filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = deque([]) \n",
    "done_folders = deque([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpath = Path(\"out\")\n",
    "# testpath = testpath.joinpath(\"\", \"test\")\n",
    "# testpath.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the folders at the root to be processed\n",
    "response = fetch_webpage(url=root_url+\"/filepro/documents/\")\n",
    "bs = BeautifulSoup(response.content,'html.parser')\n",
    "folders.extend(get_items(bs, parent_url=\"/filepro/documents/\",parent=[], is_folder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breadth-first search to find all subfolders and documents\n",
    "while len(folders)>0:\n",
    "    time.sleep(1)\n",
    "    logger.debug(\"folders to visit: %s\", \"\\n\".join([str(folder) for folder in folders]))\n",
    "    logger.debug(\"completed folders: %s\", \"\\n\".join([str(folder) for folder in done_folders]))\n",
    "    \n",
    "    curr_folder = folders.popleft()\n",
    "    logger.info(f\"\\nSearching folder {curr_folder['name']} at location /{'/'.join(curr_folder['parent'])}...\") \n",
    "    try:\n",
    "        response = fetch_webpage(url=root_url+curr_folder[\"url\"])\n",
    "        bs = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "        # update current folder\n",
    "        curr_path = curr_folder[\"parent\"].copy()\n",
    "        curr_path.append(curr_folder[\"name\"])\n",
    "        logger.debug(\"currrent folder's parents:%s\", curr_folder[\"parent\"])\n",
    "        logger.debug(\"current folder's name:%s\", curr_folder[\"name\"])\n",
    "        logger.debug(\"parent path to add to children folders/documents: %s\", curr_path)\n",
    "        \n",
    "        # add subfolders to visit from this folder to the folders deque\n",
    "        # initial_folders = len(folders)\n",
    "        children_folders = get_items(bs,parent_url=curr_folder[\"url\"], parent=curr_path, is_folder=True)\n",
    "        folders.extend(children_folders)\n",
    "\n",
    "        # add documents to download from this folder to the documents list\n",
    "        # initial_documents = len(documents)\n",
    "        children_documents = get_items(bs,parent_url=curr_folder[\"url\"], parent=curr_path, is_folder=False)\n",
    "        documents.extend(children_documents)\n",
    "        \n",
    "        logger.info(f\"Found {len(children_folders)} folders and {len(children_documents)} documents at this location.\")\n",
    "    except Exception as e:\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(tb_str)\n",
    "        continue\n",
    "\n",
    "    done_folders.append(curr_folder)\n",
    "    curr_path = []\n",
    "    \n",
    "    logger.info(f\"Completed folder {curr_folder['name']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folders)\n",
    "print(done_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://pleasantontx.civicweb.net/filepro/documents/5688/ #has 4 documents\n",
    "# bs = get_soup(url_append='/filepro/documents/5688/', root_url=root_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# documents = get_documents(bs, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for doc in bs.find_all(\"a\", class_='document-link'):\n",
    "#     document_info = {\n",
    "#         \"name\": doc.text.strip(),\n",
    "#         \"url\": doc[\"href\"],\n",
    "#         \"subdomain\": subdomain,\n",
    "#         }\n",
    "#     documents.append(document_info)\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_parent_url(document:dict, folders:deque=done_folders):\n",
    "#     return [folder for folder in folders \n",
    "#     if \"\".join(folder[\"parents\"])+folder[\"name\"] == \"\".join(document[\"parents\"])\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FOLDER = Path.cwd() / \"out\"\n",
    "rows = []\n",
    "for document in documents[:10]:\n",
    "    error = \"\"\n",
    "    file_extension = \"\"\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        response = fetch_webpage(url=root_url+document[\"url\"], headers=HEADERS)\n",
    "        t2 = time.time()\n",
    "        logger.info((f\"Took {round((t2-t1),3)} seconds to get page.\"))\n",
    "        out_path = OUT_FOLDER.joinpath(subdomain, *document[\"parent\"])\n",
    "\n",
    "        file_extension, file_type = get_filetype(response)\n",
    "        logger.info((f\"{document['name']}{file_extension}\"))\n",
    "        download_file(\n",
    "            response, \n",
    "            filename=document[\"name\"]+file_extension,\n",
    "            out_path=out_path)\n",
    "    except Exception as e:\n",
    "        tb_str = ''.join(traceback.format_exception(e))\n",
    "        logger.error(tb_str)\n",
    "        error += e # update with error\n",
    "    finally:\n",
    "        download_dict = {\n",
    "            \"name\": document[\"name\"]+file_extension,\n",
    "            \"file_type\": file_type,\n",
    "            \"subdomain\": subdomain,\n",
    "            \"parent_path\": \"/\".join(document[\"parent\"]),\n",
    "            \"root_url\": root_url, \n",
    "            \"url\": document[\"url\"], \n",
    "            \"parent_url\": document[\"parent_url\"],\n",
    "            \"time_scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"'error'\": error\n",
    "        }\n",
    "        rows.append(download_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(rows)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv(OUT_FOLDER / \"scraped_files.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = pd.read_csv(OUT_FOLDER / \"scraped_files.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = time.time()\n",
    "# response = requests.get(root_url+documents[0][\"url\"], headers=HEADERS)\n",
    "# t2 = time.time()\n",
    "# print(f\"Took {round((t2-t1),3)} seconds to get page.\")\n",
    "\n",
    "# t3 = time.time()\n",
    "# with open(\"./out/\"+documents[0][\"name\"]+\".pdf\", 'wb') as pdf:\n",
    "#     pdf.write(response.content)\n",
    "#     pdf.close()\n",
    "# t4 = time.time()\n",
    "# print(f\"Took {round((t4-t3),3)} seconds to save pdf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0] and [1] are in folder /filepro/documents/5869 \n",
    "response = fetch_webpage(url_append='/filepro/documents/5869', root_url=root_url, headers=HEADERS)\n",
    "bs = BeautifulSoup(response.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = fetch_webpage(url_append=documents[0][\"url\"], root_url=root_url, headers=HEADERS)\n",
    "bs = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(response, filename=documents[0][\"name\"]+\".html\", parents=documents[0][\"parents\"], subdomain=subdomain, out_path = Path.cwd() / \"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers['Content-Type'].split(\";\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mimetypes.guess_extension(response.headers['Content-Type'])) # results in None\n",
    "print(mimetypes.guess_extension(response.headers['Content-Type'].split(\";\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mimetypes.guess_extension('text/html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mimetypes.guess_extension(response1.headers['Content-Type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimetypes.types_map['.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = fetch_webpage(url_append=documents[1][\"url\"], root_url=root_url, headers=HEADERS)\n",
    "bs1 = BeautifulSoup(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response1.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response1.headers[\"Content-Disposition\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response1.headers.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers[\"Content-Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = fetch_webpage(url_append=documents[1][\"url\"], root_url=root_url, headers=HEADERS)\n",
    "bs = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers[\"Content-Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimetypes.guess_extension('image/jpeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "civicweb_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
